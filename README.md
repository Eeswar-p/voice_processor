# ğŸ™ï¸ Voice Processor

**AI-powered Speaker Diarization and Voice-to-Text Transcription System**

> Built with SpeechBrain, OpenAI Whisper, and Streamlit

---

## ğŸ“‹ Table of Contents
- [Overview](#overview)
- [Key Features](#key-features)
- [Project Structure](#project-structure)
- [Technical Architecture](#technical-architecture)
- [Installation & Setup](#installation--setup)
- [Usage](#usage)
- [Pipeline Workflow](#pipeline-workflow)
- [Technologies Used](#technologies-used)

---

## ğŸ¯ Overview

Voice Processor is an end-to-end AI system that performs **speaker diarization** (identifying "who spoke when") and **automatic speech recognition** (converting speech to text). The system can isolate a specific target speaker from multi-speaker audio and generate accurate transcripts.

### Use Cases
- Meeting transcription with speaker identification
- Podcast/interview processing
- Voice command extraction from conversations
- Multi-speaker audio analysis

---

## âœ¨ Key Features

1. **Target Speaker Isolation** - Identify and extract a specific speaker's voice from group conversations
2. **Voice Activity Detection (VAD)** - Multi-tier fallback system (Silero AI â†’ Energy-based)
3. **Speaker Embeddings** - Deep learning-based speaker recognition using ECAPA-TDNN
4. **Speech-to-Text** - State-of-the-art Whisper ASR for accurate transcription
5. **Multi-Format Support** - Handles WAV and MP3 audio files
6. **Web Interface** - User-friendly Streamlit UI with real-time processing
7. **Performance Optimized** - Model caching, smart segment filtering, parallel processing

---

## ğŸ“ Project Structure

```
voice-processor/
â”‚
â”œâ”€â”€ app/                          # Core Application
â”‚   â”œâ”€â”€ __init__.py              # Package init with compatibility patches
â”‚   â”œâ”€â”€ main.py                  # Pipeline orchestrator (CLI entry point)
â”‚   â”‚
â”‚   â”œâ”€â”€ audio/                   # Audio Processing Module
â”‚   â”‚   â”œâ”€â”€ __init__.py
â”‚   â”‚   â””â”€â”€ io.py               # Audio loading/saving (WAV/MP3)
â”‚   â”‚
â”‚   â”œâ”€â”€ pipeline/               # ML Pipeline Components
â”‚   â”‚   â”œâ”€â”€ __init__.py
â”‚   â”‚   â”œâ”€â”€ config.py          # Configuration dataclass
â”‚   â”‚   â”œâ”€â”€ vad.py             # Voice Activity Detection
â”‚   â”‚   â”œâ”€â”€ embedding.py       # Speaker embedding extraction
â”‚   â”‚   â”œâ”€â”€ diarization.py     # Speaker diarization logic
â”‚   â”‚   â””â”€â”€ asr.py             # Automatic Speech Recognition
â”‚   â”‚
â”‚   â”œâ”€â”€ ui/                     # User Interface
â”‚   â”‚   â”œâ”€â”€ __init__.py
â”‚   â”‚   â””â”€â”€ streamlit_app.py   # Streamlit web application
â”‚   â”‚
â”‚   â””â”€â”€ utils/                  # Utilities
â”‚       â”œâ”€â”€ __init__.py
â”‚       â””â”€â”€ logging.py          # Logging configuration
â”‚
â”œâ”€â”€ tests/                       # Unit Tests
â”‚   â”œâ”€â”€ __init__.py
â”‚   â””â”€â”€ test_audio_io.py
â”‚
â”œâ”€â”€ docs/                        # Documentation
â”‚   â””â”€â”€ Awe_hackaton.pdf       # Project requirements
â”‚
â”œâ”€â”€ outputs/                     # Generated Outputs (gitignored)
â”‚   â””â”€â”€ ui_run/
â”‚       â”œâ”€â”€ diarization.json    # Transcription with timestamps
â”‚       â”œâ”€â”€ target_speaker.wav  # Isolated target audio
â”‚       â””â”€â”€ _tmp/               # Temporary files
â”‚
â”œâ”€â”€ requirements.txt             # Python dependencies
â””â”€â”€ README.md                   # Project documentation
```

---

## ğŸ—ï¸ Technical Architecture

### Pipeline Components

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  Audio Input    â”‚ (Mixture + Target Sample)
â””â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”˜
         â”‚
         â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ Audio Loading   â”‚ â†’ Supports WAV/MP3, auto-resampling to 16kHz
â””â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”˜
         â”‚
         â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚   Embedding     â”‚ â†’ SpeechBrain ECAPA-TDNN (target speaker)
â””â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”˜
         â”‚
         â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚      VAD        â”‚ â†’ Silero (PyTorch) / Energy-based fallback
â””â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”˜
         â”‚
         â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  Diarization    â”‚ â†’ Cosine similarity-based speaker matching
â””â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”˜
         â”‚
         â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚      ASR        â”‚ â†’ OpenAI Whisper (tiny/base/small)
â””â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”˜
         â”‚
         â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ JSON + Audio    â”‚ â†’ Transcript + Isolated speaker audio
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

### Algorithm Details

1. **VAD (Voice Activity Detection)**
   - Primary: Silero VAD (PyTorch Hub)
   - Fallback: Energy-based RMS threshold
   - Output: Speech intervals [(start, end), ...]

2. **Speaker Embedding**
   - Model: SpeechBrain ECAPA-TDNN
   - Pretrained: VoxCeleb dataset
   - Output: 192-dimensional L2-normalized vector

3. **Diarization**
   - Algorithm: Cosine similarity scoring
   - Threshold: 0.6 (configurable)
   - Labels: "Target" vs "Other"

4. **ASR (Automatic Speech Recognition)**
   - Model: OpenAI Whisper
   - Modes: tiny (fast), base, small (accurate)
   - Optimization: Skip segments < 0.5s

---

## ğŸš€ Installation & Setup

### Prerequisites
- Python 3.13+
- 4GB+ RAM
- FFmpeg (for MP3 support)

### Install Dependencies

```powershell
pip install -r requirements.txt
```

### First Run (downloads models ~500MB)
Models are cached automatically on first use:
- Whisper: ~/.cache/whisper/
- SpeechBrain: ~/.cache/huggingface/
- Silero VAD: ~/.cache/torch/hub/

---

## ğŸ’» Usage

### Web Interface (Recommended)

```powershell
streamlit run app/ui/streamlit_app.py
```

Then open http://localhost:8501

**Steps:**
1. Upload multi-speaker audio file (WAV/MP3)
2. Upload target speaker sample (3-10 seconds)
3. Adjust settings (threshold, model size)
4. Click "Run Pipeline"
5. View transcript with analysis metrics

### Command Line Interface

```powershell
python -m app.main mixture.wav target.wav --out outputs/
```

**Options:**
```
--asr-model {tiny,base,small}  Whisper model (default: tiny)
--threshold FLOAT              Similarity threshold 0-1 (default: 0.6)
--device {cpu,cuda}            Processing device
```

---

## ğŸ”„ Pipeline Workflow

### Step 1: Audio Loading
- Load mixture audio (multi-speaker)
- Load target speaker sample
- Resample to 16kHz mono

### Step 2: Embedding Extraction
- Process target sample through ECAPA-TDNN
- Generate 192-dim speaker embedding
- Normalize with L2 norm

### Step 3: VAD
- Detect speech segments in mixture
- Filter non-speech regions
- Output intervals with timestamps

### Step 4: Diarization
- For each speech segment:
  - Extract embedding
  - Compute cosine similarity with target
  - Label as "Target" or "Other" based on threshold

### Step 5: ASR
- Transcribe each segment with Whisper
- Optional: Process only target speaker (faster)
- Skip segments < 0.5s for performance

### Step 6: Output Generation
- **diarization.json**: Structured transcript
  ```json
  [{
    "speaker": "Target",
    "start": 10.1,
    "end": 11.9,
    "text": "transcribed text here",
    "confidence": 0.0
  }]
  ```
- **target_speaker.wav**: Concatenated target audio

---

## ğŸ› ï¸ Technologies Used

| Component | Technology | Purpose |
|-----------|------------|---------|
| Deep Learning | PyTorch 2.9 | Neural network framework |
| Speaker Recognition | SpeechBrain 1.0 | ECAPA-TDNN embeddings |
| Speech Recognition | OpenAI Whisper | Voice-to-text |
| VAD | Silero VAD | Speech detection |
| Audio I/O | soundfile, librosa | File handling |
| Web UI | Streamlit 1.50 | Interactive interface |
| Language | Python 3.13 | Core implementation |

---

## ğŸ“Š Performance Metrics

- **VAD Accuracy**: 3-5 seconds for 15-min audio
- **Embedding Speed**: ~30-50s per target sample
- **ASR Speed**: ~1 segment/second (CPU, tiny model)
- **Total Processing**: ~2-5 min for 15-min audio (target-only mode)

### Optimization Features
âœ… Model caching (no reloading)  
âœ… Skip short segments (< 0.5s)  
âœ… Target-only transcription mode  
âœ… Efficient audio resampling  

---

## ğŸ“ Interview Talking Points

1. **Problem Solved**: Multi-speaker audio transcription with speaker identification
2. **ML Models Used**:
   - ECAPA-TDNN for speaker embeddings
   - Silero VAD for speech detection
   - Whisper for transcription
3. **Key Challenges**:
   - Model compatibility across SpeechBrain versions
   - Performance optimization for real-time processing
   - Audio format handling (WAV/MP3)
4. **Technical Decisions**:
   - Cosine similarity for speaker matching
   - Multi-tier VAD fallback
   - Modular pipeline design
5. **Results**: Accurate speaker isolation + transcription with timestamps

---

## ğŸ“„ License

Educational/Hackathon Project

---

## ğŸ™ Credits

- **SpeechBrain** - Speaker recognition models
- **OpenAI** - Whisper ASR
- **Silero Team** - VAD model
- **Streamlit** - UI framework
