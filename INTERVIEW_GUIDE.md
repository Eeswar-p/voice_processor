# ğŸ¤ Voice Processor - Interview Guide

## Quick Project Summary
**Voice Processor** is an AI-powered system that performs speaker diarization (identifying who spoke when) and automatic speech recognition (converting speech to text). It can isolate a specific target speaker from multi-speaker audio and generate accurate transcripts.

---

## ğŸ¯ Key Points to Mention

### 1. Problem Statement
"I built a voice processing system that solves two problems:
1. **Speaker Diarization** - Identifying which speaker said what in multi-speaker audio
2. **Transcription** - Converting the isolated speech to text with timestamps"

### 2. Technologies Used
- **SpeechBrain ECAPA-TDNN** - Deep learning model for speaker recognition
- **OpenAI Whisper** - State-of-the-art speech-to-text
- **Silero VAD** - Voice activity detection
- **Streamlit** - Web interface
- **PyTorch** - Deep learning framework

### 3. Technical Approach

**Step-by-step pipeline:**
1. Load audio files (mixture + target speaker sample)
2. Extract speaker embedding from target using ECAPA-TDNN (192-dim vector)
3. Run VAD to detect speech intervals
4. For each segment, compute embedding and compare with target (cosine similarity)
5. Label segments as "Target" or "Other" based on similarity threshold (0.6)
6. Transcribe segments using Whisper ASR
7. Output JSON transcript with timestamps

### 4. Challenges & Solutions

| Challenge | Solution |
|-----------|----------|
| Model compatibility issues | Created dynamic import resolver + torchaudio shims |
| Slow processing | Implemented model caching, skip short segments, target-only mode |
| Multiple audio formats | Multi-tier audio loader (soundfile â†’ librosa â†’ wave) |
| Temp file cleanup | Changed from auto-delete to manual cleanup in ASR |

### 5. Code Architecture

**Modular Design:**
- `app/audio/` - Audio I/O handling
- `app/pipeline/` - ML pipeline (VAD, embeddings, diarization, ASR)
- `app/ui/` - Streamlit web interface
- Clean separation of concerns

**Key Design Patterns:**
- Singleton pattern for model caching
- Fallback pattern for VAD (Silero â†’ Energy-based)
- Configuration dataclass for settings

### 6. Performance Optimizations

âœ… **Model Caching** - Load once, reuse (Whisper, SpeechBrain, Silero)  
âœ… **Smart Filtering** - Skip segments < 0.5s  
âœ… **Target-Only Mode** - Process 80% fewer segments  
âœ… **Efficient Resampling** - Lazy audio resampling  

**Results:** 15-min audio processes in ~3 minutes (target-only mode, CPU)

### 7. Testing & Validation

- Unit tests for audio I/O
- Manual testing with real meeting recordings
- Verified transcription accuracy against ground truth
- Performance benchmarking on different audio lengths

---

## ğŸ“Š Demo Flow for Interview

### If asked to demonstrate:

1. **Show Streamlit UI**
   ```powershell
   streamlit run app/ui/streamlit_app.py
   ```

2. **Walk through upload process**
   - Upload multi-speaker audio
   - Upload target speaker sample (3-10s)
   - Explain threshold setting

3. **Explain processing steps** (point to logs)
   - Audio loading time
   - Embedding computation
   - VAD intervals detected
   - Diarization results (X target, Y other segments)
   - Transcription progress

4. **Show results**
   - Analysis metrics
   - Timestamped transcript
   - Download options

---

## ğŸ—£ï¸ Talking Points by Topic

### Machine Learning
- "I used ECAPA-TDNN for speaker embeddings - it's a state-of-the-art CNN architecture"
- "Cosine similarity for matching speakers - simple but effective"
- "Whisper is OpenAI's robust ASR model, I used the 'tiny' variant for speed"

### Software Engineering
- "Modular architecture with clear separation of pipeline stages"
- "Implemented fallback mechanisms for robustness"
- "Comprehensive error handling and logging"
- "Configuration management with dataclasses"

### Problem Solving
- "When SpeechBrain imports failed, I created a dynamic import resolver"
- "Optimized from 10min to 3min processing time by implementing caching"
- "Added multi-format support when MP3 uploads were needed"

### Future Improvements
- "Real-time streaming support"
- "Multi-speaker tracking (3+ speakers)"
- "GPU acceleration for faster processing"
- "Speaker embedding fine-tuning on domain-specific data"

---

## ğŸ“ Code Snippets to Highlight

### 1. Speaker Similarity (diarization.py)
```python
def cosine_sim(a: np.ndarray, b: np.ndarray) -> float:
    return float(np.dot(a, b) / ((np.linalg.norm(a) + 1e-9) * 
                                  (np.linalg.norm(b) + 1e-9)))
```

### 2. VAD Fallback (vad.py)
```python
try:
    return _vad_silero(wav, sr, frame_ms)  # Primary
except:
    return _vad_energy(wav, sr, frame_ms)  # Fallback
```

### 3. Model Caching (asr.py)
```python
_WHISPER_MODELS = {}

def _get_whisper_model(model_size: str):
    if model_size not in _WHISPER_MODELS:
        _WHISPER_MODELS[model_size] = whisper.load_model(model_size)
    return _WHISPER_MODELS[model_size]
```

---

## ğŸ“ Questions You Might Get

**Q: How does speaker diarization work?**  
A: Extract embeddings from target sample, compare each segment's embedding using cosine similarity, label based on threshold.

**Q: Why did you choose these specific models?**  
A: ECAPA-TDNN is SOTA for speaker verification, Whisper is robust to accents/noise, Silero VAD is fast and accurate.

**Q: How do you handle performance?**  
A: Model caching, skip short segments, optional target-only mode, efficient resampling, preloading in UI.

**Q: What if there are 3+ speakers?**  
A: Current binary classification. Could extend to clustering (K-means on embeddings) or supervised multi-class.

**Q: How accurate is it?**  
A: Depends on audio quality. Clean audio: ~90%+ diarization, ~95%+ transcription. Noisy audio: degrades but still functional.

---

## ğŸ“ Project Structure (Quick Reference)

```
app/
â”œâ”€â”€ main.py          # Pipeline orchestrator
â”œâ”€â”€ audio/io.py      # Audio loading (WAV/MP3)
â”œâ”€â”€ pipeline/
â”‚   â”œâ”€â”€ config.py    # Settings
â”‚   â”œâ”€â”€ vad.py       # Voice detection
â”‚   â”œâ”€â”€ embedding.py # Speaker embeddings
â”‚   â”œâ”€â”€ diarization.py # Speaker matching
â”‚   â””â”€â”€ asr.py       # Whisper transcription
â””â”€â”€ ui/streamlit_app.py  # Web interface
```

---

## ğŸ’¡ Pro Tips

1. **Start with the problem** - Multi-speaker transcription is relatable
2. **Mention real use cases** - Meetings, podcasts, interviews
3. **Highlight technical depth** - Neural networks, embeddings, similarity metrics
4. **Show trade-offs** - Speed vs accuracy (tiny vs small model)
5. **Demonstrate live** - Running demo is impressive

---

## â±ï¸ Time Estimates
- Project explanation: 3-5 minutes
- Architecture walkthrough: 2-3 minutes
- Live demo: 2-3 minutes
- Code deep-dive: 5-10 minutes (if asked)

**Total: 10-20 minutes** depending on depth

---

Good luck with your interview! ğŸš€
